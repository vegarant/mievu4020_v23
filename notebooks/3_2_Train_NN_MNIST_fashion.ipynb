{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and create dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Dataset\n",
    "We can index Datasets manually like a list: `training_data[index]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape:  torch.Size([1, 28, 28])\n",
      "img.dtype:  torch.float32\n",
      "img.device:  cpu\n",
      "im1.shape: torch.Size([28, 28])\n",
      "im2.shape:  torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "sample_idx = 108\n",
    "\n",
    "img, label = training_data[sample_idx]\n",
    "print('img.shape: ',  img.shape)\n",
    "print('img.dtype: ',  img.dtype)\n",
    "print('img.device: ', img.device) # Notice that the data is lying on the CPU. This is standard.\n",
    "\n",
    "im1 = img.squeeze()\n",
    "print('im1.shape:', im1.size())\n",
    "im2 = im1.unsqueeze(0)\n",
    "print('im2.shape: ', im2.shape)\n",
    "\n",
    "plt.title(labels_map[label], fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders\n",
    "The `Dataset` retrieves our dataset’s features and labels one sample at a time. When training a model, we typically want to pass samples in “minibatches” and reshuffle the data at every epoch to reduce model overfitting and use Python’s `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "shuffle=True\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=shuffle)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iterate through the DataLoader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_input, train_labels = next(iter(train_dataloader)) # Convert to iterable and get next element\n",
    "print(f\"Input batch shape: {train_input.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_input[0].squeeze()\n",
    "label = int(train_labels[0])\n",
    "\n",
    "plt.title(labels_map[label], fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 60, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(60, 15, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (3): ReLU()\n",
      "  (4): Flatten(start_dim=1, end_dim=-1)\n",
      "  (5): Linear(in_features=540, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=60, kernel_size=3, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=60, out_channels=15, kernel_size=3,stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=540, out_features=10, bias=True)\n",
    "    )\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 60, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(60, 15, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (3): ReLU()\n",
       "  (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  (5): Linear(in_features=540, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a few hyperparameters\n",
    "device = torch.device('cpu')\n",
    "epochs = 5 # Number of iterations with stochastic gradient descent.\n",
    "learning_rate = 0.003\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Define the loss funciton. It does not expect the input to be normalized\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Choose the optimizer \n",
    "model.to(device) # Move model parameters to the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the train loop\n",
    "\n",
    "Inside the training loop, optimization happens in three steps:\n",
    " * Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    " * Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    " * Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    batch = 0\n",
    "    for x, y in dataloader:\n",
    "        # Compute prediction and loss\n",
    "        batch = batch + 1\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # A common mistake is to forget to call this function.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def validation_loop(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            val_loss += loss_fn(pred, y).item() # This is a float\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validataion Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.273598  [ 6400/60000]\n",
      "loss: 2.236412  [12800/60000]\n",
      "loss: 2.124992  [19200/60000]\n",
      "loss: 1.948927  [25600/60000]\n",
      "loss: 1.562172  [32000/60000]\n",
      "loss: 1.244132  [38400/60000]\n",
      "loss: 0.961538  [44800/60000]\n",
      "loss: 0.904058  [51200/60000]\n",
      "loss: 1.106958  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.828420 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.802708  [ 6400/60000]\n",
      "loss: 0.780256  [12800/60000]\n",
      "loss: 0.612196  [19200/60000]\n",
      "loss: 0.678949  [25600/60000]\n",
      "loss: 0.669241  [32000/60000]\n",
      "loss: 0.544007  [38400/60000]\n",
      "loss: 0.669057  [44800/60000]\n",
      "loss: 0.719986  [51200/60000]\n",
      "loss: 0.892313  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.692046 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.632110  [ 6400/60000]\n",
      "loss: 0.694546  [12800/60000]\n",
      "loss: 0.664383  [19200/60000]\n",
      "loss: 0.410873  [25600/60000]\n",
      "loss: 0.716733  [32000/60000]\n",
      "loss: 0.516055  [38400/60000]\n",
      "loss: 0.625669  [44800/60000]\n",
      "loss: 0.563766  [51200/60000]\n",
      "loss: 0.660221  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.633002 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.591928  [ 6400/60000]\n",
      "loss: 0.769384  [12800/60000]\n",
      "loss: 0.740307  [19200/60000]\n",
      "loss: 0.592479  [25600/60000]\n",
      "loss: 0.693752  [32000/60000]\n",
      "loss: 0.324310  [38400/60000]\n",
      "loss: 0.615427  [44800/60000]\n",
      "loss: 0.623565  [51200/60000]\n",
      "loss: 0.755180  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.599765 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.581539  [ 6400/60000]\n",
      "loss: 0.498746  [12800/60000]\n",
      "loss: 0.631939  [19200/60000]\n",
      "loss: 0.391102  [25600/60000]\n",
      "loss: 0.445457  [32000/60000]\n",
      "loss: 0.551350  [38400/60000]\n",
      "loss: 0.695669  [44800/60000]\n",
      "loss: 0.414510  [51200/60000]\n",
      "loss: 0.444108  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.578621 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    validation_loop(test_dataloader, model, loss_fn, device)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
